{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **CLIP MODEL Implementation**\n",
        "\n",
        "- Multi-Head Attention (custom implementation, original repo used PyTorch's nn.MultiheadAttention)\n",
        "- Transformer Architecture\n",
        "- Text Transformer\n",
        "- Vision Transformer\n",
        "- Contrastive Loss\n",
        "\n",
        "Acknowledgement: [CLIP's Repository](https://github.com/openai/CLIP/tree/main)"
      ],
      "metadata": {
        "id": "JjbuY-RXJIf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from typing import Any, Optional, Tuple, Union\n",
        "\n",
        "!pip install ftfy regex tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jj-YjVVHJHV_",
        "outputId": "18ccb37b-ca97-474f-a103-5c5d940ad260"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = tokenizer.encode(text)\n",
        "  return tokens\n",
        "\n",
        "def tokenize_text(texts, context_len): # [B, \"text string\"]\n",
        "  if isinstance(texts, str):\n",
        "    texts = [texts]\n",
        "\n",
        "  batch_tokens = []\n",
        "  sot_token = tokenizer.encoder[\"<|startoftext|>\"]\n",
        "  eot_token = tokenizer.encoder[\"<|endoftext|>\"]\n",
        "  batch_tokens = [[sot_token] + tokenize(text) + [eot_token] for text in texts]\n",
        "\n",
        "  result = torch.zeros(len(batch_tokens), context_len, dtype=torch.int)\n",
        "\n",
        "  for i, tokens in enumerate(batch_tokens):\n",
        "    if len(tokens) > context_len:\n",
        "      tokens = tokens[:context_len]\n",
        "      tokens[-1] = eot_token\n",
        "    result[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "zWbENleGkc0v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuickGELU(nn.Module):\n",
        "  def forward(self, x):\n",
        "    return x * torch.sigmoid(1.702 * x)"
      ],
      "metadata": {
        "id": "FqMlKR4brIHC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MHAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, attn_mask=None):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model # d_model/n_heads = 64 (head_dim) from \"Attention is all you need\"\n",
        "      self.num_heads = num_heads\n",
        "      self.head_dim = d_model // num_heads\n",
        "\n",
        "      # D = 64 * num_heads\n",
        "      # d = 64\n",
        "      # h = num_heads\n",
        "      # L = seq_len\n",
        "\n",
        "      if self.head_dim * self.num_heads != self.d_model:\n",
        "        raise ValueError(\n",
        "            f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.d_model} and `num_heads`:\"\n",
        "            f\" {self.num_heads}).\"\n",
        "        )\n",
        "\n",
        "      self.scale = self.head_dim**-0.5 # sqrt(512/8=64)\n",
        "      self.dropout = 0.1\n",
        "\n",
        "      self.q_proj = nn.Linear(d_model, d_model)\n",
        "      self.k_proj = nn.Linear(d_model, d_model)\n",
        "      self.v_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "      self.out_proj = nn.Linear(d_model, d_model) # first d_model = d_v * n_heads\n",
        "\n",
        "    def _shape(self, tensor, seq_len, bsz):\n",
        "      # return N,h,L,d tensor\n",
        "      return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1,2).contiguous()\n",
        "\n",
        "    def forward(self, x, attn_mask=None, output_attentions=True): # NLD\n",
        "\n",
        "      # attn_mask : size()-> (bsz, 1, L, L)\n",
        "\n",
        "      bsz, ctx_len, embed_dim = x.size()\n",
        "\n",
        "      queries = self.q_proj(x) * self.scale # NLD\n",
        "\n",
        "      keys = self.k_proj(x) # NLD\n",
        "      keys = self._shape(keys, -1, bsz) # NhLd\n",
        "\n",
        "      values = self.v_proj(x)\n",
        "      values = self._shape(values, -1, bsz) # NhLd\n",
        "\n",
        "      proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "\n",
        "      queries = self._shape(queries, ctx_len, bsz).view(*proj_shape) # N*h, L, d\n",
        "      keys = keys.view(*proj_shape) # N*h, L, d\n",
        "      values = values.view(*proj_shape) # N*h, L, d\n",
        "\n",
        "      src_len = keys.size(1)\n",
        "\n",
        "      # keys.transpose(1,2) # N*h, d, L\n",
        "      attn_weights = torch.bmm(queries, keys.transpose(1,2)) # N*h, L, L\n",
        "\n",
        "      if attn_mask is not None:\n",
        "        # adds -inf to values where we don't want to put attention to\n",
        "        attn_weights = attn_weights.view(bsz, self.num_heads, ctx_len, ctx_len) + attn_mask\n",
        "        attn_weights = attn_weights.view(bsz * self.num_heads, ctx_len, ctx_len) # N*h, L, L\n",
        "\n",
        "      attn_weights = nn.functional.softmax(attn_weights, dim=-1) # softmax(-inf) -> 0 -> attention is zero for that token\n",
        "\n",
        "      if output_attentions:\n",
        "        # From CLIP's repo: this operation is a bit akward, but it's required to\n",
        "        # make sure that attn_weights keeps its gradient.\n",
        "        # In order to do so, attn_weights have to reshaped\n",
        "        # twice and have to be reused in the following\n",
        "        attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, ctx_len, ctx_len)\n",
        "        attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, ctx_len, ctx_len)\n",
        "      else:\n",
        "        attn_weights_reshaped = None\n",
        "\n",
        "      # N*h, L, L\n",
        "      attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "      # values: N*h, L, d\n",
        "      attn_outputs = torch.bmm(attn_probs, values) # N*h, L, d\n",
        "\n",
        "      attn_outputs = attn_outputs.contiguous().view(bsz, self.num_heads, ctx_len, self.head_dim)\n",
        "      attn_outputs = attn_outputs.transpose(1,2)\n",
        "      attn_outputs = attn_outputs.contiguous().view(bsz, ctx_len, self.d_model)\n",
        "\n",
        "      attn_outputs = self.out_proj(attn_outputs)\n",
        "      return attn_outputs, attn_weights_reshaped\n",
        ""
      ],
      "metadata": {
        "id": "8sdinOG59tav"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualAttentionBlock(nn.Module):\n",
        "  def __init__(self, d_model, n_heads, attn_mask=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attn_block = MHAttention(d_model, n_heads)\n",
        "    self.ln_1 = nn.LayerNorm(d_model)\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(d_model, 4*d_model),\n",
        "        QuickGELU(),\n",
        "        nn.Linear(4*d_model, d_model)\n",
        "    )\n",
        "    self.ln_2 = nn.LayerNorm(d_model)\n",
        "    self.attn_mask = attn_mask\n",
        "\n",
        "  def attention(self, x):\n",
        "    self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
        "    return self.attn_block(x, attn_mask=self.attn_mask, output_attentions=False)[0]\n",
        "\n",
        "  def forward(self, x): # NLD\n",
        "    res = x\n",
        "    x = self.ln_1(x)\n",
        "    x = self.attention(x)\n",
        "    x += res\n",
        "    res = x\n",
        "    x = self.ln_2(x)\n",
        "    x = self.mlp(x)\n",
        "    x += res\n",
        "    return x\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               width,\n",
        "               layers,\n",
        "               heads,\n",
        "               attn_mask=None,\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.model = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask=attn_mask) for _ in range(layers)])\n",
        "\n",
        "  def forward(self, x): # x: NLD\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "I_z4yedW913-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextTransformer(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size,\n",
        "               context_len,\n",
        "               width,\n",
        "               layers,\n",
        "               heads,\n",
        "               output_dim,\n",
        "               attn_mask=None):\n",
        "    super().__init__()\n",
        "    self.token_embedding = nn.Embedding(vocab_size, width)\n",
        "    self.position_embedding = nn.Parameter(torch.empty(context_len, width))\n",
        "\n",
        "    self.transformer = Transformer(width, layers, heads, attn_mask)\n",
        "\n",
        "    self.ln_final = nn.LayerNorm(width)\n",
        "    self.text_projection = nn.Parameter(torch.empty(width, output_dim))\n",
        "\n",
        "\n",
        "  def forward(self, tokens):\n",
        "    x = self.token_embedding(tokens)\n",
        "    x = x + self.position_embedding # NLD\n",
        "\n",
        "    x = self.transformer(x) # NLD\n",
        "    x = self.ln_final(x) # # NLD\n",
        "\n",
        "    # extracting eot_token features (representative of entire input sentence)\n",
        "    x = x[torch.arange(x.shape[0]), tokens.argmax(dim=-1)] # ND\n",
        "    x = x @ self.text_projection # projecting to CLIP embedding space\n",
        "\n",
        "    return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, input_resolution, patch_size, width, layers, heads, output_dim):\n",
        "    super().__init__()\n",
        "    self.input_resolution = input_resolution\n",
        "    self.patch_size = patch_size\n",
        "    self.output_dim = output_dim\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
        "\n",
        "    self.scale = width ** -0.5\n",
        "    self.width = width\n",
        "    self.class_embedding = nn.Parameter(self.scale * torch.randn(width)) # CLS token for an input\n",
        "    self.positional_embedding = nn.Parameter(self.scale * torch.randn((input_resolution // patch_size)**2 + 1, width))\n",
        "    # self.ln_pre = nn.LayerNorm(width) # redundant as Transformer apply LN to input\n",
        "\n",
        "    self.transformer = Transformer(width, layers, heads)\n",
        "    self.ln_post = nn.LayerNorm(width)\n",
        "    self.proj = nn.Parameter(self.scale * torch.randn(width, output_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x) # bsz, width, patch_size, patch_size\n",
        "    x = x.view(x.shape[0], x.shape[1], -1) # bsz, width, n_patches\n",
        "    x = x.permute(0, 2, 1) # bsz, n_patches, width (n_patches == seq_len)\n",
        "    x = torch.cat([torch.zeros(x.shape[0], 1, x.shape[-1]) + self.class_embedding, x], dim=1) # bsz, n_patches+1, width\n",
        "    x = x + self.positional_embedding\n",
        "    # x = self.ln_pre(x)\n",
        "\n",
        "    x = self.transformer(x) # bsz, n_patches+1, width\n",
        "    x = self.ln_post(x[:, 0, :]) # # bsz, width\n",
        "    x = x @ self.proj\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "nx5kDi8vrB4M"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_causal_attn_mask(ctx_len):\n",
        "  mask = torch.empty(ctx_len, ctx_len)\n",
        "  mask.fill_(float('-inf'))\n",
        "  mask.triu_(1)\n",
        "  return mask\n",
        "\n",
        "class CLIP(nn.Module):\n",
        "  def __init__(self,\n",
        "               vocab_size, # text\n",
        "               context_len,\n",
        "               text_width,\n",
        "               text_layers,\n",
        "               text_heads,\n",
        "               text_output_dim,\n",
        "               attn_mask,\n",
        "               image_resolution,\n",
        "               patch_size,\n",
        "               vit_width,\n",
        "               vit_layers,\n",
        "               vit_heads,\n",
        "               vit_output_dim\n",
        "               ):\n",
        "    super().__init__()\n",
        "\n",
        "    self.text_transformer = TextTransformer(vocab_size, context_len, text_width, text_layers, text_heads, text_output_dim, attn_mask)\n",
        "    self.vit = VisionTransformer(image_resolution, patch_size, vit_width, vit_layers, vit_heads, vit_output_dim)\n",
        "\n",
        "  def encode_text(self, tokens): # batch of tokens (tensors)\n",
        "    x = self.text_transformer(tokens)\n",
        "    return x\n",
        "\n",
        "  def encode_image(self, images): # batch of images (tensors)\n",
        "    return self.vit(images)\n"
      ],
      "metadata": {
        "id": "Iv5-sRlOQS1M"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from PIL import Image\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "try:\n",
        "    from torchvision.transforms import InterpolationMode\n",
        "    BICUBIC = InterpolationMode.BICUBIC\n",
        "except ImportError:\n",
        "    BICUBIC = Image.BICUBIC\n"
      ],
      "metadata": {
        "id": "a9eRfD_tjRxk"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _convert_to_rgb(image):\n",
        "  return image.convert(\"RGB\")\n",
        "\n",
        "def _transform(image_res):\n",
        "  return Compose([\n",
        "      Resize(image_res, interpolation=BICUBIC),\n",
        "      CenterCrop(image_res),\n",
        "      _convert_to_rgb,\n",
        "      ToTensor(),\n",
        "      Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "  ])"
      ],
      "metadata": {
        "id": "nc_ortcfHR7f"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import html\n",
        "import os\n",
        "from functools import lru_cache\n",
        "\n",
        "import ftfy\n",
        "import regex as re\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def default_bpe():\n",
        "    return \"bpe_simple_vocab_16e6.txt.gz\"\n",
        "\n",
        "\n",
        "@lru_cache()\n",
        "def bytes_to_unicode():\n",
        "    \"\"\"\n",
        "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
        "    The reversible bpe codes work on unicode strings.\n",
        "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
        "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
        "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
        "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
        "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
        "    \"\"\"\n",
        "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
        "    cs = bs[:]\n",
        "    n = 0\n",
        "    for b in range(2**8):\n",
        "        if b not in bs:\n",
        "            bs.append(b)\n",
        "            cs.append(2**8+n)\n",
        "            n += 1\n",
        "    cs = [chr(n) for n in cs]\n",
        "    return dict(zip(bs, cs))\n",
        "\n",
        "\n",
        "def get_pairs(word):\n",
        "    \"\"\"Return set of symbol pairs in a word.\n",
        "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
        "    \"\"\"\n",
        "    pairs = set()\n",
        "    prev_char = word[0]\n",
        "    for char in word[1:]:\n",
        "        pairs.add((prev_char, char))\n",
        "        prev_char = char\n",
        "    return pairs\n",
        "\n",
        "\n",
        "def basic_clean(text):\n",
        "    text = ftfy.fix_text(text)\n",
        "    text = html.unescape(html.unescape(text))\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def whitespace_clean(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "class SimpleTokenizer(object):\n",
        "    def __init__(self, bpe_path: str = default_bpe()):\n",
        "        self.byte_encoder = bytes_to_unicode()\n",
        "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
        "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
        "        merges = merges[1:49152-256-2+1]\n",
        "        merges = [tuple(merge.split()) for merge in merges]\n",
        "        vocab = list(bytes_to_unicode().values()) # bytes/symbols\n",
        "        vocab = vocab + [v+'</w>' for v in vocab] # including when bytes/symbols are at the end of word\n",
        "        for merge in merges:\n",
        "            vocab.append(''.join(merge)) # I think merging most frequently occuring pairs\n",
        "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
        "        self.vocab = vocab\n",
        "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
        "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
        "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
        "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
        "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
        "\n",
        "    def bpe(self, token):\n",
        "        if token in self.cache:\n",
        "            return self.cache[token]\n",
        "\n",
        "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
        "        pairs = get_pairs(word)\n",
        "        if not pairs:\n",
        "            return token+'</w>'\n",
        "        while True:\n",
        "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
        "            if bigram not in self.bpe_ranks:\n",
        "                break\n",
        "            first, second = bigram\n",
        "\n",
        "            new_word = []\n",
        "            i = 0\n",
        "            while i < len(word):\n",
        "                try:\n",
        "                    j = word.index(first, i)\n",
        "                    new_word.extend(word[i:j])\n",
        "                    i = j\n",
        "                except:\n",
        "                    new_word.extend(word[i:])\n",
        "                    break\n",
        "\n",
        "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
        "                    new_word.append(first+second)\n",
        "                    i += 2\n",
        "                else:\n",
        "                    new_word.append(word[i])\n",
        "                    i += 1\n",
        "\n",
        "            new_word = tuple(new_word)\n",
        "            word = new_word\n",
        "            if len(word) == 1:\n",
        "                break\n",
        "            else:\n",
        "                pairs = get_pairs(word)\n",
        "\n",
        "        word = ' '.join(word)\n",
        "        self.cache[token] = word\n",
        "        return word\n",
        "\n",
        "    def encode(self, text):\n",
        "        bpe_tokens = []\n",
        "        text = whitespace_clean(basic_clean(text)).lower()\n",
        "        for token in re.findall(self.pat, text):\n",
        "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
        "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
        "        return bpe_tokens\n",
        "\n",
        "    def decode(self, tokens):\n",
        "        text = ''.join([self.decoder[token] for token in tokens])\n",
        "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
        "        return text"
      ],
      "metadata": {
        "id": "r1gnhMOfCMfp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text): # \"text string\" -> list of tokens\n",
        "  tokens = tokenizer.encode(text)\n",
        "  return tokens\n",
        "\n",
        "def tokenize_text(texts, context_len): # [B, \"text string\"] or \"text string\"\n",
        "  if isinstance(texts, str):\n",
        "    texts = [texts]\n",
        "\n",
        "  batch_tokens = []\n",
        "  sot_token = tokenizer.encoder[\"<|startoftext|>\"]\n",
        "  eot_token = tokenizer.encoder[\"<|endoftext|>\"]\n",
        "  batch_tokens = [[sot_token] + tokenize(text) + [eot_token] for text in texts]\n",
        "\n",
        "  result = torch.zeros(len(batch_tokens), context_len, dtype=torch.int)\n",
        "\n",
        "  for i, tokens in enumerate(batch_tokens):\n",
        "    if len(tokens) > context_len:\n",
        "      tokens = tokens[:context_len]\n",
        "      tokens[-1] = eot_token\n",
        "    result[i, :len(tokens)] = torch.tensor(tokens)\n",
        "\n",
        "  return result"
      ],
      "metadata": {
        "id": "qMCX9iv5RH8Q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# text\n",
        "tokenizer = SimpleTokenizer()\n",
        "vocab_size = len(tokenizer.vocab)\n",
        "context_len = 50\n",
        "width = 256\n",
        "heads = 8\n",
        "layers = 6\n",
        "output_dim = 512\n",
        "\n",
        "tokenizer = SimpleTokenizer()\n",
        "\n",
        "# image\n",
        "image_resolution = 224\n",
        "patch_size = 16\n",
        "\n",
        "image_preprocess = _transform(image_resolution)\n",
        "\n",
        "clip = CLIP(vocab_size, context_len, width, layers, heads, output_dim, build_causal_attn_mask(context_len),\n",
        "            image_resolution, patch_size, width, layers, heads, output_dim)"
      ],
      "metadata": {
        "id": "FPjGDOYdR6X4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"a diagram\", \"a dog\", \"a cat\"]\n",
        "batch_tokens = tokenize_text(texts, context_len)\n",
        "clip_text_embedding = clip.encode_text(batch_tokens)\n",
        "print('CLIP Text Embeddings:', clip_text_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKN0L7CnR6f-",
        "outputId": "430c30f6-58d6-4949-b45f-475f139f4f61"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP Text Embeddings: torch.Size([3, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = image_preprocess(Image.open(\"CLIP.png\")).unsqueeze(0)\n",
        "clip_image_embedding = clip.encode_image(image)\n",
        "print('CLIP image embeddings:', clip_image_embedding.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "isR8r9vdZTfd",
        "outputId": "840d4718-21a4-4f95-f288-0b4b5ab6e450"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLIP image embeddings: torch.Size([1, 512])\n"
          ]
        }
      ]
    }
  ]
}